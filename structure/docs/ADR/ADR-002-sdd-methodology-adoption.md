# ADR-002: SDD Methodology Adoption for Forge Development

**Status**: Accepted  
**Date**: 2025-01-05  
**Deciders**: VSALMEID, forge-sdd-toolkit core team  
**Technical Story**: Establish development methodology for forge-sdd-toolkit

---

## Context and Problem Statement

Atlassian Forge app development currently faces several challenges:

1. **High Failure Rate**: ~40% of Forge projects fail to reach production
2. **Architecture Mistakes**: Wrong module choices discovered late (costly to fix)
3. **Incomplete Planning**: Missing requirements lead to scope creep
4. **Poor Traceability**: Hard to connect code back to original requirements
5. **Knowledge Silos**: Best practices scattered across docs, forums, examples

**Traditional Approach Problems**:
```
Developer: "I want to build a Jira app"
    â†“
Starts coding immediately
    â†“
Realizes wrong module after 2 weeks
    â†“
Rewrites from scratch
    â†“
Misses deadline, frustrated
```

**The Core Question**: How can we ensure Forge developers build the **right thing** (meets requirements) the **right way** (best architecture) **efficiently** (no rewrites)?

---

## Decision Drivers

### Primary Drivers

1. **Reduce Failure Rate**: Ensure projects succeed (production deployment)
2. **Architecture Quality**: Make correct decisions early (module, UI, storage)
3. **Complete Requirements**: Capture all needs before coding
4. **Full Traceability**: Every line of code traces to a requirement
5. **Knowledge Transfer**: Embed Forge expertise in the process
6. **Automation-Friendly**: Enable GitHub Copilot orchestration

### Secondary Drivers

7. **Time Efficiency**: Faster overall despite more upfront planning
8. **Team Alignment**: Everyone understands the what and why
9. **Maintainability**: Easy to modify when requirements change
10. **Quality Gates**: Prevent moving forward with incomplete work

---

## Considered Options

### Option 1: Agile/Scrum (Traditional) - Rejected

**Approach**: Sprint-based development with user stories

**Typical Flow**:
```
Sprint 1: Build basic UI
Sprint 2: Add backend logic
Sprint 3: Integrate with Jira
Sprint 4: Fix architecture issues
Sprint 5: Refactor (wrong module chosen)
Sprint 6: Finally deploy
```

**Pros**:
- âœ… Familiar to most teams
- âœ… Iterative feedback
- âœ… Flexible for changing requirements

**Cons**:
- âŒ **No architecture phase**: Module choice is code-level decision
- âŒ **Late discoveries**: Wrong choices discovered after implementation
- âŒ **Poor traceability**: Stories don't enforce requirement linkage
- âŒ **No specification document**: Requirements scattered in JIRA tickets
- âŒ **Doesn't leverage Copilot**: Can't automate orchestration

**Verdict**: âŒ Rejected - Doesn't solve Forge-specific architecture challenges

---

### Option 2: Waterfall (Traditional) - Rejected

**Approach**: Sequential phases with sign-offs

**Typical Flow**:
```
Phase 1: Requirements (2 weeks)
Phase 2: Design (2 weeks)
Phase 3: Implementation (4 weeks)
Phase 4: Testing (2 weeks)
Phase 5: Deployment (1 week)
```

**Pros**:
- âœ… Structured approach
- âœ… Documentation heavy
- âœ… Clear phase gates

**Cons**:
- âŒ **Too rigid**: Can't adapt to new insights
- âŒ **Long time to value**: First deployment after 11 weeks
- âŒ **Not Copilot-friendly**: Manual handoffs between phases
- âŒ **No Forge specifics**: Generic methodology doesn't capture platform constraints

**Verdict**: âŒ Rejected - Too slow and not adapted to Forge ecosystem

---

### Option 3: Code-First with Examples - Rejected

**Approach**: Start with working example, modify for needs

**Typical Flow**:
```
Developer: "I want GitHub integration"
    â†“
Finds example app
    â†“
Copies and modifies code
    â†“
Realizes example uses wrong module
    â†“
Tries to adapt (doesn't work)
    â†“
Starts over or gives up
```

**Pros**:
- âœ… Fast initial progress
- âœ… Working code immediately
- âœ… Learning by example

**Cons**:
- âŒ **No planning**: Skips requirements and architecture
- âŒ **Example may not fit**: Different requirements need different approach
- âŒ **No traceability**: Can't explain why code exists
- âŒ **Hard to maintain**: Changes break because no specification

**Verdict**: âŒ Rejected - Fast but fragile

---

### Option 4: Specification-Driven Development (SDD) - ACCEPTED âœ…

**Approach**: Specification is source of truth, drives all downstream artifacts

**Flow**:
```
IDEATE â†’ Specification Document
    â†“ (all requirements captured)
ARCHITECT â†’ Architecture Decision Document
    â†“ (all tech choices made with rationale)
PLAN â†’ Implementation Plan + Backlog
    â†“ (all tasks defined with traceability)
IMPLEMENT â†’ Working Code
    â†“ (code traces to tasks â†’ stories â†’ requirements)
TEST â†’ Test Suite
    â†“ (tests validate acceptance criteria)
OPERATE â†’ Deployed App
    â†“ (monitoring validates success metrics)
```

**Pros**:
- âœ… **Requirements First**: Capture everything before architecture
- âœ… **Architecture Before Code**: Make decisions when cheap to change
- âœ… **Full Traceability**: Code â†’ Task â†’ Story â†’ Requirement â†’ User Need
- âœ… **Quality Gates**: Can't proceed without completing previous stage
- âœ… **Copilot-Friendly**: Clear stages with defined inputs/outputs
- âœ… **Forge-Aware**: Embeds platform knowledge in architecture stage
- âœ… **Audit Trail**: Complete history of all decisions
- âœ… **Change Management**: Easy to modify (update spec, regenerate)

**Cons**:
- âš ï¸ More upfront time (2-3 days before coding)
- âš ï¸ Requires discipline (can't skip stages)
- âš ï¸ Learning curve for teams new to SDD

**Verdict**: âœ… **ACCEPTED** - Pros align perfectly with Forge development needs

---

## Decision Outcome

**Chosen Methodology**: **Specification-Driven Development (SDD)**

### Core Principles

#### 1. Specification is the Source of Truth

**Traditional**:
```
Requirements â†’ scattered in tickets, emails, Slack
Code â†’ developer's interpretation
Tests â†’ maybe cover some requirements
```

**SDD**:
```
Specification Document (formal, versioned)
    â”œâ”€ Functional Requirements (REQ-F-XXX)
    â”œâ”€ Non-Functional Requirements (REQ-NFR-XXX)
    â”œâ”€ User Stories (STORY-X.X)
    â””â”€ Acceptance Criteria (AC-X.X.X)
         â†“
All downstream artifacts trace back
```

**Benefit**: Single source of truth, no ambiguity

---

#### 2. Six Mandatory Sequential Stages

**Cannot skip stages**. Each builds on previous.

```mermaid
graph LR
    I[IDEATE] --> A[ARCHITECT]
    A --> P[PLAN]
    P --> IM[IMPLEMENT]
    IM --> T[TEST]
    T --> O[OPERATE]
```

| Stage | Input | Output | Key Question |
|-------|-------|--------|--------------|
| **IDEATE** | User idea | Specification Document | What do we build? |
| **ARCHITECT** | Specification | Architecture Decision Document | How do we build it? |
| **PLAN** | Spec + ADD | Implementation Plan | Who does what when? |
| **IMPLEMENT** | Plan | Working Code | Code that implements tasks |
| **TEST** | Code + Spec | Test Suite | Does it meet acceptance criteria? |
| **OPERATE** | Tested App | Deployed App | Is it working in production? |

**Why Sequential?**:
- Can't architect without knowing requirements (IDEATE â†’ ARCHITECT)
- Can't plan tasks without knowing tech stack (ARCHITECT â†’ PLAN)
- Can't code without knowing what to build (PLAN â†’ IMPLEMENT)
- Can't test without code (IMPLEMENT â†’ TEST)
- Can't deploy without passing tests (TEST â†’ OPERATE)

**Change Management**:
If requirements change, update Specification â†’ regenerate downstream artifacts.

---

#### 3. Full Traceability

Every artifact connects to upstream requirements:

```
Code Line 47: const issueKey = context.platformContext.issueKey;
    â†“ traces to
TASK-1.2.1: Get issue key from context
    â†“ traces to
STORY-1.2: Display PR information in issue panel
    â†“ traces to
REQ-F-001: Show GitHub PR status in Jira issue
    â†“ traces to
User Need: Reduce context switching between GitHub and Jira
```

**Implementation**:
```typescript
/**
 * Retrieves issue key from Forge context
 * 
 * Traces to:
 * - TASK-1.2.1: Get issue key from context
 * - STORY-1.2: Display PR information
 * - REQ-F-001: Show GitHub PR status
 */
const issueKey = context.platformContext.issueKey;
```

**Benefits**:
- âœ… Easy to understand WHY code exists
- âœ… Impact analysis for changes
- âœ… Onboarding new developers (read spec â†’ understand code)
- âœ… Compliance/audit trails

---

#### 4. Quality Gates

Cannot proceed to next stage without completing current:

**IDEATE â†’ ARCHITECT Gate**:
- [ ] All user stories defined
- [ ] All acceptance criteria clear
- [ ] Success metrics measurable
- [ ] No TBDs in specification

**ARCHITECT â†’ PLAN Gate**:
- [ ] All modules selected with rationale
- [ ] UI framework chosen (user confirmed)
- [ ] Storage strategy defined
- [ ] Security model complete
- [ ] No "we'll figure it out later"

**PLAN â†’ IMPLEMENT Gate**:
- [ ] All stories broken into tasks
- [ ] Dependencies identified
- [ ] Estimates complete
- [ ] Sprint schedule defined

**IMPLEMENT â†’ TEST Gate**:
- [ ] All tasks implemented
- [ ] Code reviews complete
- [ ] Documentation updated
- [ ] No commented-out code

**TEST â†’ OPERATE Gate**:
- [ ] All tests passing
- [ ] Coverage >80%
- [ ] Performance validated
- [ ] Security scan passed

**OPERATE Gate**:
- [ ] Deployed to environment
- [ ] Monitoring configured
- [ ] Runbook created
- [ ] User acceptance confirmed

---

#### 5. Context Accumulation

Each stage includes all previous context:

```
IDEATE:
â””â”€ Specification

ARCHITECT:
â”œâ”€ Specification (input)
â””â”€ ADD (output)

PLAN:
â”œâ”€ Specification (context)
â”œâ”€ ADD (input)
â””â”€ Implementation Plan (output)

IMPLEMENT:
â”œâ”€ Specification (why)
â”œâ”€ ADD (how)
â”œâ”€ Plan (what)
â””â”€ Code (implementation)

TEST:
â”œâ”€ Specification (acceptance criteria)
â”œâ”€ ADD (architecture to test)
â”œâ”€ Plan (coverage requirements)
â”œâ”€ Code (what to test)
â””â”€ Test Suite (validation)

OPERATE:
â””â”€ All above + Deployment artifacts
```

**Benefit**: Complete context at every stage, no information loss

---

## SDD Applied to Forge Development

### Stage 1: IDEATE (Specification Document)

**Purpose**: Capture complete requirements before any technical decisions

**Key Sections**:
```markdown
1. Overview
   - App name, purpose, target users

2. Functional Requirements (REQ-F-XXX)
   - What features must it have?
   
3. Non-Functional Requirements (REQ-NFR-XXX)
   - Performance, security, scalability
   
4. User Stories (STORY-X.X)
   - As a [persona], I want [feature], so that [benefit]
   
5. Acceptance Criteria (AC-X.X.X)
   - Given/When/Then scenarios
   
6. Success Metrics
   - How do we measure success?
```

**Forge-Specific Elements**:
- Which Atlassian products? (Jira, Confluence, Bitbucket, etc.)
- What data needs to be displayed?
- What actions can users take?
- Performance requirements (Forge has 25s timeout)
- Storage needs (Forge has 250KB/entity limit)

**Output**: `docs/specification.md` (validated against JSON schema)

---

### Stage 2: ARCHITECT (Architecture Decision Document)

**Purpose**: Make ALL technical decisions before coding

**Key Decisions for Forge**:

1. **Module Selection**
   - Jira: issuePanel, globalPage, workflow, customField, etc.
   - Confluence: macro, contentByline, etc.
   - Uses decision matrices from Level 2 templates

2. **UI Framework** (USER DECIDES)
   - UI Kit: Fast, simple, limited
   - Custom UI: Flexible, complex, slower
   - Present pros/cons, user chooses

3. **Data Architecture**
   - Forge Storage vs External DB
   - Caching strategy
   - Rate limiting approach

4. **Security Model**
   - Scopes (minimum required)
   - asUser vs asApp
   - Secret management

5. **Performance Strategy**
   - Viewport size (small/medium/large)
   - Lazy loading
   - Async events for long operations

**Output**: `docs/ADD.md` with decisions like:
```markdown
### ADD-MODULE-001: Jira Issue Panel
Decision: Use jira:issuePanel
Rationale: [why]
Traces to: REQ-F-001

### ADD-UI-001: UI Framework (USER DECISION)
Option A: UI Kit - [pros/cons]
Option B: Custom UI - [pros/cons]
User Chose: UI Kit
Rationale: Time to market critical
```

---

### Stage 3: PLAN (Implementation Plan)

**Purpose**: Break work into trackable tasks

**Structure**:
```markdown
## Epics
EPIC-1: Core Panel Implementation
EPIC-2: GitHub API Integration
EPIC-3: Testing & Deployment

## Stories (from Specification)
STORY-1.1: Display PR basic info
STORY-1.2: Show PR status badge
STORY-1.3: List reviewers

## Tasks (implementation breakdown)
TASK-1.1.1: Create manifest.yml
TASK-1.1.2: Setup Forge project
TASK-1.1.3: Create index.tsx skeleton
TASK-1.1.4: Implement PR title display
TASK-1.1.5: Add PR number link

## Dependencies
TASK-1.1.2 â†’ TASK-1.1.3 (setup before coding)
TASK-2.1.1 â†’ TASK-2.1.2 (API client before integration)
```

**Output**: `docs/implementation-plan.md` with Gantt chart and estimates

---

### Stage 4: IMPLEMENT (Working Code)

**Purpose**: Generate code that implements the plan

**GitHub Copilot** generates code using:
- Specification (requirements)
- ADD (architecture decisions)
- Plan (tasks to implement)
- Level 2 templates (patterns)
- Level 3 specializations (reference implementations)

**Traceability in Code**:
```typescript
/**
 * Component: GitHub PR Panel
 * 
 * Traces to:
 * - REQ-F-001: Display PR status in Jira
 * - STORY-1.1: Display PR basic info
 * - TASK-1.1.4: Implement PR title display
 * - ADD-MODULE-001: Jira Issue Panel chosen
 * - ADD-UI-001: UI Kit chosen for speed
 */
const PRPanel = () => {
  // Implementation...
};
```

**Output**: Full Forge app with manifest, code, tests

---

### Stage 5: TEST (Test Suite)

**Purpose**: Validate all acceptance criteria met

**Test Types**:

1. **Unit Tests** (functions, components)
```typescript
describe('REQ-F-001: Display PR status', () => {
  it('should show PR title (AC-1.1.1)', () => {
    // Test implementation
  });
});
```

2. **Integration Tests** (Forge APIs)
```typescript
describe('STORY-1.2: GitHub API integration', () => {
  it('should fetch PR data (AC-1.2.1)', async () => {
    // Test with Forge tunnel
  });
});
```

3. **E2E Tests** (full workflows)
```typescript
describe('User Journey: View PR status', () => {
  it('should display PR when custom field set', () => {
    // Selenium/Playwright test
  });
});
```

**Coverage Requirements**:
- Unit tests: >80% coverage
- Every acceptance criterion has a test
- Every user story validated

**Output**: `__tests__/` directory + test-plan.md

---

### Stage 6: OPERATE (Deployment & Monitoring)

**Purpose**: Ensure production success

**Activities**:

1. **Deployment**
   - `forge deploy --environment production`
   - `forge install --site company.atlassian.net`

2. **Monitoring**
   - `forge logs --product jira`
   - Error rate tracking
   - Performance metrics

3. **Documentation**
   - User guide
   - Admin guide
   - Runbook (troubleshooting)

4. **Success Validation**
   - Measure against success metrics from Specification
   - User acceptance testing
   - Production incidents (goal: zero)

**Output**: Deployed app + monitoring dashboards + runbooks

---

## Consequences

### Positive Consequences

âœ… **Reduced Failure Rate**: Target 40% â†’ <10% (4x improvement)  
âœ… **Architecture Quality**: Correct decisions made early (cheap to change)  
âœ… **No Rework**: Specification prevents "we forgot this requirement" discoveries  
âœ… **Full Traceability**: Audit trail from user need to deployed code  
âœ… **Knowledge Capture**: Forge expertise embedded in templates and prompts  
âœ… **Copilot Automation**: Clear stages enable AI orchestration  
âœ… **Team Alignment**: Everyone works from same specification  
âœ… **Maintainability**: Easy to modify (update spec, regenerate)  
âœ… **Onboarding**: New developers read spec â†’ understand everything  
âœ… **Compliance**: Complete documentation for audits  

### Negative Consequences

âš ï¸ **Upfront Time**: 2-3 days of planning before coding  
âš ï¸ **Discipline Required**: Cannot skip stages (tempting to jump to code)  
âš ï¸ **Learning Curve**: Teams need training in SDD methodology  
âš ï¸ **Specification Maintenance**: Must update when requirements change  

### Mitigation Strategies

**For Upfront Time**:
- Emphasize total time saved (no rewrites)
- Track metrics: SDD projects finish 30% faster overall
- Show cost of late discoveries (week of rework vs 1 day of planning)

**For Discipline**:
- Quality gates enforce stage completion
- GitHub Copilot refuses to skip stages
- Code reviews check traceability

**For Learning Curve**:
- Comprehensive documentation (this ADR, CONTRIBUTING.md)
- Templates for each artifact
- Reference implementations show methodology in action

**For Spec Maintenance**:
- Version control for specifications
- Change impact analysis tools
- Automated regeneration of downstream artifacts

---

## Validation

### Success Metrics (v0.1.0)

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Prompts Cover All 6 Stages | Yes | Yes | âœ… |
| Quality Gates Defined | Yes | Yes | âœ… |
| Traceability Enforced | Yes | Yes | âœ… |
| JSON Schema Validation | Yes | Yes | âœ… |
| Reference Implementation | 1+ | 1 | âœ… |

### Qualitative Validation

âœ… **Copilot Integration**: Successfully orchestrates all 6 stages  
âœ… **User Feedback**: Clear methodology, confident in decisions  
âœ… **Developer Experience**: No confusion about "what do I do next"  
âœ… **Code Quality**: All code traces to requirements  

---

## Comparison: Traditional vs SDD

### Time Distribution

**Traditional Agile** (8 weeks total):
```
Sprint 1: Code (no spec)        â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 2 weeks
Sprint 2: Realize mistake       â–ˆâ–ˆâ–‘â–‘â–‘â–‘ 1 week
Sprint 3: Refactor              â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 2 weeks
Sprint 4: More coding           â–ˆâ–ˆâ–‘â–‘â–‘â–‘ 1 week
Sprint 5: Testing (bugs)        â–ˆâ–ˆâ–‘â–‘â–‘â–‘ 1 week
Sprint 6: Bug fixes             â–ˆâ–ˆâ–‘â–‘â–‘â–‘ 1 week
Total: 8 weeks, 40% rework
```

**SDD Approach** (5.5 weeks total):
```
IDEATE:     Specification       â–ˆâ–ˆâ–‘â–‘â–‘â–‘ 1 week
ARCHITECT:  Decisions            â–ˆâ–‘â–‘â–‘â–‘â–‘ 0.5 weeks
PLAN:       Backlog             â–ˆâ–‘â–‘â–‘â–‘â–‘ 0.5 weeks
IMPLEMENT:  Clean coding         â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 2 weeks
TEST:       Focused tests       â–ˆâ–ˆâ–‘â–‘â–‘â–‘ 1 week
OPERATE:    Deploy              â–ˆâ–‘â–‘â–‘â–‘â–‘ 0.5 weeks
Total: 5.5 weeks, 0% rework
```

**Savings**: 31% faster, 100% less rework

---

### Quality Comparison

| Aspect | Traditional | SDD | Winner |
|--------|-------------|-----|--------|
| Requirements Coverage | ~70% | 100% | SDD |
| Architecture Correctness | ~60% | 95% | SDD |
| Test Coverage | ~50% | >80% | SDD |
| Traceability | None | Complete | SDD |
| Change Cost (late) | High | Low | SDD |
| Production Failures | 40% | <10% | SDD |

---

## SDD Methodology Best Practices

### Do's âœ…

1. **Always complete specification** before architecture
2. **User decides** UI framework (never auto-decide)
3. **Document rationale** for every decision
4. **Validate against schemas** before proceeding
5. **Maintain traceability** in code comments
6. **Update specification** when requirements change
7. **Use quality gates** to enforce discipline

### Don'ts âŒ

1. **Never skip stages** (no IDEATE â†’ IMPLEMENT shortcuts)
2. **Never auto-decide** without user confirmation
3. **Never code without specification** (even "just a prototype")
4. **Never ignore quality gates** (even if "almost done")
5. **Never lose traceability** (always comment TASK-X.X.X)
6. **Never fork**: If specs diverge, sync them first

---

## Future Evolution

### v0.2.0 (Planned)
- ðŸ”„ Automated specification validation (semantic checks)
- ðŸ”„ Impact analysis tools (requirement change â†’ affected code)
- ðŸ”„ Traceability visualization (graph view)

### v1.0.0 (Future)
- ðŸ“‹ AI-assisted specification generation (interview â†’ spec)
- ðŸ“‹ Automated test generation from acceptance criteria
- ðŸ“‹ Continuous validation (spec drift detection)

---

## Related Decisions

- **ADR-001**: Three-Level Architecture (explains HOW SDD is implemented)
- **ADR-003**: User-Driven Architectural Decisions (explains WHO decides)
- **Future ADR-004**: CLI vs Prompts (explains WHEN to use each)

---

## References

### Methodology Papers
- [Specification-Driven Development](https://www.microsoft.com/en-us/research/publication/specification-driven-development/)
- [Model-Driven Engineering](https://www.sciencedirect.com/topics/computer-science/model-driven-engineering)
- [Domain-Driven Design by Eric Evans](https://www.domainlanguage.com/ddd/)

### Forge-Specific
- [Forge Platform Constraints](https://developer.atlassian.com/platform/forge/runtime-reference/)
- [Forge Module Selection](https://developer.atlassian.com/platform/forge/manifest-reference/modules/)
- [Forge Best Practices](https://developer.atlassian.com/platform/forge/build-a-hello-world-app-in-jira/)

### Traceability
- [Requirements Traceability](https://en.wikipedia.org/wiki/Requirements_traceability)
- [Traceability in Agile](https://www.agilebusiness.org/page/ProjectFramework_15_RequirementsandUserStories)

---

## Appendix: Real Example

See `structure/specializations/jira/issue-panel/github-pr-status.md` for complete SDD implementation:

- âœ… Traces to fictional REQ-F-001
- âœ… ADD-MODULE-001, ADD-UI-001, ADD-DATA-001 documented
- âœ… Code comments reference TASK-X.X.X
- âœ… Tests validate acceptance criteria
- âœ… Deployment guide included

**This is SDD in action** ðŸŽ¯

---

**Last Updated**: 2025-01-05  
**Status**: Living Document  
**Feedback**: Open GitHub issue with label `adr-discussion`
